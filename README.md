# Fast Trembling Line Optimizer (FTLO)

FTLO is a optimizer for training neural networks

FTLOæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ä¼˜åŒ–å™¨ç®—æ³•

## Formule | å…¬å¼ï¼š

### 1. Update of the momentum variable $v$ | åŠ¨é‡å˜é‡ $v$ çš„æ›´æ–°ï¼š

$$
v \leftarrow \beta_2 v + (\beta_2 \cdot \gamma) g
$$

### 2. Update of the parameters $\theta$ | å‚æ•° $\theta$ çš„æ›´æ–°ï¼š

$$
\theta \leftarrow \theta - \eta g - \alpha v
$$

### 3. Definition of dynamic coefficients | åŠ¨æ€ç³»æ•°å®šä¹‰ï¼š

Here, the coefficients $\alpha$ and $\gamma$ decay with the time step $t$ (starting from 1):

å…¶ä¸­ï¼Œç³»æ•° $\alpha$ å’Œ $\gamma$ éšæ—¶é—´æ­¥ $t$ï¼ˆä» $1$ å¼€å§‹ï¼‰è¿›è¡Œè¡°å‡ï¼š

* **Momentum coefficient $\alpha$** | **åŠ¨é‡ç³»æ•° $\alpha$**ï¼š

$$
\alpha = \frac{\beta_1}{\eta \cdot (t+1)^P}
$$

* **Learning rate $\gamma$ for $v$ update** | **$v$ æ›´æ–°å­¦ä¹ ç‡ $\gamma$**ï¼š

$$
\gamma = \frac{\eta}{(t+1)^Q}
$$

$g \text{ is the gradient, and } t \text{ is the current time step.}$

$g \text{ ä¸ºæ¢¯åº¦ï¼Œ} t \text{ ä¸ºå½“å‰æ—¶é—´æ­¥ã€‚}$

## âš™ï¸ Hyperparameter Settings | è¶…å‚æ•°è®¾ç½®

We identified a parameter combination that outperforms Adam through comparative experiments with fixed random seeds on the MNIST task

æˆ‘ä»¬åœ¨ MNIST ä»»åŠ¡ä¸Šï¼Œé€šè¿‡å›ºå®šéšæœºç§å­å¯¹æ¯”å®éªŒï¼Œæ‰¾åˆ°äº†è¶…è¶Š Adam çš„å‚æ•°ç»„åˆ

| Parameter | Symbol | Description | Value |
| :--- | :--- | :--- | :--- |
| å‚æ•° | ç¬¦å· | æè¿° | æ•°å€¼ |
| Initial Learning Rate | $\eta$ | - | $1 \times 10^{-3}$ |
| åˆå§‹å­¦ä¹ ç‡ | $\eta$ | - | $1 \times 10^{-3}$ |
| Momentum Coefficient 1 | $\beta_1$ | Baseline affecting $\alpha$ | $0.8$ |
| åŠ¨é‡ç³»æ•° 1 | $\beta_1$ | å½±å“ $\alpha$ çš„åŸºå‡† | $0.8$ |
| $v$ Decay Factor | $\beta_2$ | History retention rate for $v$ | $0.98$ |
| $v$ è¡°å‡ç³»æ•° | $\beta_2$ | $v$ çš„å†å²ä¿ç•™ç‡ | $0.98$ |
| Momentum Decay Exponent | $P$ | Decay speed for $\alpha$ | $0.4$ |
| åŠ¨é‡è¡°å‡æŒ‡æ•° | $P$ | è¡°å‡é€Ÿåº¦ï¼ˆ $\alpha$ ï¼‰ | $0.4$ |
| $v$ Decay Exponent | $Q$ | Decay speed for $\gamma$ | $0.2$ |
| $v$ è¡°å‡æŒ‡æ•° | $Q$ | è¡°å‡é€Ÿåº¦ï¼ˆ $\gamma$ ï¼‰ | $0.2$ |

## âš ï¸ Robustness Recommendation: Gradient Clipping | é²æ£’æ€§å»ºè®®ï¼šæ¢¯åº¦è£å‰ª

Since the momentum term $\alpha v$ and the $v$ update learning rate $\gamma$ in the FTLO may lead to aggressive update steps during the early stages of training, we **strongly recommend** applying gradient clipping before optimizer.step() to ensure numerical stability across various tasks (especially those prone to gradient explosion).

ç”±äºFTLOçš„åŠ¨é‡é¡¹ $\alpha v$ å’Œ $v$ æ›´æ–°å­¦ä¹ ç‡ $\gamma$ åœ¨è®­ç»ƒåˆæœŸå¯èƒ½å¯¼è‡´æ¿€è¿›çš„æ›´æ–°æ­¥éª¤ï¼Œä¸ºäº†ç¡®ä¿åœ¨å„ç§ä»»åŠ¡ï¼ˆå°¤å…¶å…·æœ‰æ¢¯åº¦çˆ†ç‚¸é£é™©çš„ä»»åŠ¡ï¼‰ä¸Šçš„æ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬**å¼ºçƒˆå»ºè®®**åœ¨ `optimizer.step()` ä¹‹å‰åº”ç”¨æ¢¯åº¦è£å‰ª

**In our MNIST experiments, we used CLIP_NORM = 1.0**

**åœ¨ MNIST å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† CLIP_NORM = 1.0**

## ğŸ Pictures | å›¾åƒ

![mnist_seed42](./images/ftlo_mnist_rs42.png)
![mnist_seed3407](./images/ftlo_mnist_rs3407.png)

![rosenbrockfunc](./images/rb.png)
