# Fast Trembling Line Optimizer (FTLO)

FTLO is a optimizer for training neural networks

FTLOæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ä¼˜åŒ–å™¨ç®—æ³•

## Formule | å…¬å¼ï¼š

### 1. Update of the momentum variable $v$ | åŠ¨é‡å˜é‡ $v$ çš„æ›´æ–°ï¼š

$$
v \leftarrow \beta_2 v + (\beta_2 \cdot \gamma) g
$$

### 2. Update of the parameters $\theta$ | å‚æ•° $\theta$ çš„æ›´æ–°ï¼š

$$
\theta \leftarrow \theta - \eta g - \alpha v
$$

### 3. Definition of dynamic coefficients | åŠ¨æ€ç³»æ•°å®šä¹‰ï¼š

Here, the coefficients $\alpha$ and $\gamma$ decay with the time step $t$ (starting from 1):

å…¶ä¸­ï¼Œç³»æ•° $\alpha$ å’Œ $\gamma$ éšæ—¶é—´æ­¥ $t$ï¼ˆä» $1$ å¼€å§‹ï¼‰è¿›è¡Œè¡°å‡ï¼š

* **Momentum coefficient $\alpha$** | **åŠ¨é‡ç³»æ•° $\alpha$**ï¼š

$$
\alpha = \frac{\beta_1}{\eta \cdot (t+1)^P}
$$

* **Learning rate $\gamma$ for $v$ update** | **$v$ æ›´æ–°å­¦ä¹ ç‡ $\gamma$**ï¼š

$$
\gamma = \frac{\eta}{(t+1)^Q}
$$

$g \text{ is the gradient, and } t \text{ is the current time step.}$

$g \text{ ä¸ºæ¢¯åº¦ï¼Œ} t \text{ ä¸ºå½“å‰æ—¶é—´æ­¥ã€‚}$

## âš™ï¸ Hyperparameter Settings | è¶…å‚æ•°è®¾ç½®

We identified a parameter combination that outperforms Adam through comparative experiments with fixed random seeds on the MNIST task

æˆ‘ä»¬åœ¨ MNIST ä»»åŠ¡ä¸Šï¼Œé€šè¿‡å›ºå®šéšæœºç§å­å¯¹æ¯”å®éªŒï¼Œæ‰¾åˆ°äº†è¶…è¶Š Adam çš„å‚æ•°ç»„åˆ

| Parameter/å‚æ•° | Symbol/ç¬¦å· | Description/æè¿° | Value/æ•°å€¼ |
| :--- | :--- | :--- | :--- |
| Initial Learning Rate | $\eta$ | - | $1 \times 10^{-3}$ |
| åˆå§‹å­¦ä¹ ç‡ | $\eta$ | - | $1 \times 10^{-3}$ |
| Momentum Coefficient 1 | $\beta_1$ | Baseline affecting $\alpha$ | $0.8$ |
| åŠ¨é‡ç³»æ•° 1 | $\beta_1$ | å½±å“ $\alpha$ çš„åŸºå‡† | $0.8$ |
| $v$ Decay Factor | $\beta_2$ | History retention rate for $v$ | $0.98$ |
| $v$ è¡°å‡ç³»æ•° | $\beta_2$ | $v$ çš„å†å²ä¿ç•™ç‡ | $0.98$ |
| Momentum Decay Exponent | $P$ | Decay speed for $\alpha$ | $0.4$ |
| åŠ¨é‡è¡°å‡æŒ‡æ•° | $P$ | è¡°å‡é€Ÿåº¦ï¼ˆ $\alpha$ ï¼‰ | $0.4$ |
| $v$ Decay Exponent | $Q$ | Decay speed for $\gamma$ | $0.2$ |
| $v$ è¡°å‡æŒ‡æ•° | $Q$ | è¡°å‡é€Ÿåº¦ï¼ˆ $\gamma$ ï¼‰ | $0.2$ |

## ğŸ“ Tuning Strategy and Robustness Observations | è°ƒä¼˜ç­–ç•¥ä¸é²æ£’æ€§è§‚å¯Ÿ

The FTLO performs well in tasks **far from the optimal solution** or those that are **high-dimensional and complex** (such as MNIST). However, when the initial point is **very close to the optimum** (for example, starting at $(0, 0)$ for the Rosenbrock function), its momentum term may cause severe oscillations.

FTLOåœ¨**è¿œç¦»æœ€ä¼˜è§£**æˆ–**é«˜ç»´ã€å¤æ‚**ä»»åŠ¡ï¼ˆå¦‚ MNISTï¼‰ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“åˆå§‹ç‚¹**éå¸¸æ¥è¿‘æœ€ä¼˜è§£**ï¼ˆä¾‹å¦‚ Rosenbrock å‡½æ•°çš„ $(0, 0)$ èµ·ç‚¹ï¼‰æ—¶ï¼Œå…¶åŠ¨é‡é¡¹å¯èƒ½å¯¼è‡´å‰§çƒˆæŒ¯è¡ã€‚

**Adjusting the decay parameters according to the initial conditions is key to using FTLO:**

**æ ¹æ®åˆå§‹æ¡ä»¶è°ƒæ•´è¡°å‡å‚æ•°ï¼Œæ˜¯ä½¿ç”¨ FTLO çš„å…³é”®ï¼š**

| Scenario/åœºæ™¯ | Initial Conditions/åˆå§‹æ¡ä»¶ | Tuning Suggestion/è°ƒå‚å»ºè®® | Purpose/ç›®çš„ |
| :--- | :--- | :--- | :--- |
| **High-Dim / Exploration** | Starting point far from target | Keep $P$, $Q$ small; $B_1$, $B_2$ moderate | Traverse flat regions quickly |
| **é«˜ç»´/æ¢ç´¢** | åˆå§‹ç‚¹è¿œç¦»ç›®æ ‡ | ä¿æŒ $P, Q$ è¾ƒå°; $B_1, B_2$ é€‚ä¸­ | å¿«é€Ÿç©¿æ¢­å¹³å¦åŒºåŸŸ |
| **Fine-Tuning / Stabilization** | Starting point close to target | **Lower $B_1$, $B_2$**; **Increase $P$, $Q$** | Weaken momentum influence, accelerate into stable convergence |
| **å¾®è°ƒ/ç¨³å®š** | åˆå§‹ç‚¹æ¥è¿‘ç›®æ ‡ | **è°ƒä½ $B_1, B_2$**; **è°ƒé«˜ $P, Q$** | å‰Šå¼±åŠ¨é‡å½±å“ï¼ŒåŠ é€Ÿè¿›å…¥ç¨³å®šæ”¶æ•› |

## âš ï¸ Robustness Recommendation: Gradient Clipping | é²æ£’æ€§å»ºè®®ï¼šæ¢¯åº¦è£å‰ª

Since the momentum term $\alpha v$ and the $v$ update learning rate $\gamma$ in the FTLO may lead to aggressive update steps during the early stages of training, we **strongly recommend** applying gradient clipping before optimizer.step() to ensure numerical stability across various tasks (especially those prone to gradient explosion).

ç”±äºFTLOçš„åŠ¨é‡é¡¹ $\alpha v$ å’Œ $v$ æ›´æ–°å­¦ä¹ ç‡ $\gamma$ åœ¨è®­ç»ƒåˆæœŸå¯èƒ½å¯¼è‡´æ¿€è¿›çš„æ›´æ–°æ­¥éª¤ï¼Œä¸ºäº†ç¡®ä¿åœ¨å„ç§ä»»åŠ¡ï¼ˆå°¤å…¶å…·æœ‰æ¢¯åº¦çˆ†ç‚¸é£é™©çš„ä»»åŠ¡ï¼‰ä¸Šçš„æ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬**å¼ºçƒˆå»ºè®®**åœ¨ `optimizer.step()` ä¹‹å‰åº”ç”¨æ¢¯åº¦è£å‰ª

**In our MNIST experiments, we used CLIP_NORM = 1.0**

**åœ¨ MNIST å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† CLIP_NORM = 1.0**

## ğŸ Pictures | å›¾åƒ

![mnist_seed42](./images/ftlo_mnist_rs42.png)
![mnist_seed3407](./images/ftlo_mnist_rs3407.png)

![rosenbrockfunc](./images/rb.png)
![rosenbrockfunc2](./images/rb2.png)
